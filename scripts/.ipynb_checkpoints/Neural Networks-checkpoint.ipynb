{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear Classifier Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.9, 5.9, 7.9, 9.9, 11.9, 13.9, 15.9, 17.9, 19.9, 21.9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHONJREFUeJzt3Xu8lWPex/HPpTCTsylntZFKomiT1GAjZchhmoyRJsdm\ncniYiUQH6UCKlFCTqRzKoZEweCi1ESntDioVCSXSAal0bl/PH7/ds9up9mGtva57rfv7fr32a+11\nt2r9Xveub/e67uv6Xc57j4iIpL89QhcgIiLJoUAXEckQCnQRkQyhQBcRyRAKdBGRDKFAFxHJEAp0\nEZEMoUAXEckQCnQRkQxRMZVvVrlyZZ+VlZXKtxQRSXvTpk1b6b2vUtzrUhroWVlZ5OXlpfItRUTS\nnnNuUUlepyEXEZEMoUAXEckQCnQRkQyhQBcRyRAKdBGRDKFAFxHJEAp0EZEMoUAXESlHS5fC7bfD\nqlXl/14KdBGRcrBmDdx7L1SvDo8/DhMnlv97KtBFRJJo82YYNMiCvHt3uPhimDcPmjcv//dO6dJ/\nEZFM5T288gp07Aiffw5nnQWvvQYNGqSuBl2hi4gk6MMPoXFj+OMfoUIFC/J3301tmIMCXUSkzD77\nzEK8cWP46it48kmYNcuGV5xLfT0KdBGRUlq2DG66CU48EcaNgx49YMECuOEGqBhwIFtj6CIiJbR2\nLTz8MPTtCxs3wt//Dl27wiGHhK7MKNBFRIqxZQsMHWrTEJctgz/9Ce6/H44/PnRlRSnQRUR2wXu7\nwdmxI8yfb2PlY8ZAw4ahK9s5jaGLiOzE5Mk29fCyywqnJL7/fnTDHBToIiJFLFhgQyoNG9r3//oX\nzJkDl14aZuZKaWjIRUQEWL7cVnb+61+w995w333wz3/CvvuGrqzkFOgiEmu//AL9+kGfPrB+PbRt\nazc/Dz00dGWlp0AXkVjasgWGD7fwXrrUFgjdfz/UrBm6srIrdgzdOXe0cy7XOTfXOfepc+62guMH\nO+fGOecWFDweVP7liogkxnv473/h5JPtavyYY+CDD2D06PQOcyjZTdEtQHvvfW3gDOBm51xtoCMw\n3nt/PDC+4LmIBDJyJGRlwR572OPIkfGuY2emTIFzzoFLLoGtW+Hlly3MGzUKXVmSeO9L9QW8CjQB\nPgMOLzh2OPBZcb+3fv36XkSSb8QI7ytV8t6uP+2rUiU7Hsc6drRggfctW1o9hxzi/RNPeL9pU9ia\nSgPI8yXIZ2evLRnnXBbwPlAHWOy9P7DguAN+2vZ8V7Kzs31eXl5p/88RkWJkZcGiRb8+Xq0afP11\n/OrYZsUK67MyaBDstRfceSe0bw/77Zf6WhLhnJvmvc8u7nUlvinqnNsXGA3c7r1f7babkOm99865\nnf7P4JxrC7QFqFq1aknfTkRKYfHi0h3P9DrWrYP+/aF3b/v+hhvs5ufhh6e2jlQr0cIi59yeWJiP\n9N6/XHB4mXPu8IJfPxxYvrPf670f4r3P9t5nV6lSJRk1i8gOdnWtlOprqNB1bN0Kw4ZZj5VOneDc\nc2H2bBg8OPPDHEo2y8UBQ4F53vt+2/3Sa0Cbgu/bYGPrIhJAr15QqVLRY5Uq2fE41OE9vPEG1K0L\n119v/4FMnGjL9U84oXzfO0pKcoXeCGgNnOucm1nw9QegN9DEObcAOL/guYgE0KoVDBliY9XO2eOQ\nIXY80+vIy7Mr8Ysvtpa2L70EkyZZI624KdVN0UTppqiIJMuXX9qwygsvQJUqNkbeti3suWfoypIv\n6TdFRUSiYOVK6NkTnnjCwrtzZ5u9sv/+oSsLT4EuImlh/XoYMAAeeMB2Drr+eujWDY44InRl0aFA\nF5FI27oVnn0WunSBJUtsA+bevaF27dCVRY/6oYtIJHkPb70Fp5wC115r0w7ffdd2EFKY75wCXUQi\nZ/p0aNIELrzQFgaNGmV9WM4+O3Rl0aZAF5HI+Oorm+JYvz588gk8+ijMnQstW0Z/t6Ao0Bi6iAT3\n44+2+Oixx6xL4z33QIcOcMABoStLLwp0EQlmwwYYONA2lli9Gq65xrZ+O+qo0JWlJw25iEjK5efD\nM89AjRp2JX7mmTBzJgwdqjBPhAJdRFJq7Fg49VRo0wYOOQQmTLA+LCedFLqy9KdAF0lQlHfoiZIZ\nM+CCC6BpUxteef55+PhjyMkJXVnm0Bi6SAJGjrT+IevW2fNFi+w5pL4xVlQtWmSLgkaMgIMOgkce\ngXbtYO+9Q1eWeXSFLpKATp0Kw3ybdevseNz99JP1WKlZE/7zHxsrX7gQbr9dYV5edIUukoCo7NAT\nJRs2wOOP2zTEVatsrLx7dzj66NCVZT5doYskIPQOPVGSn29DULVqwR13wOmn28yV4cMV5qmiQBdJ\nQFR2CgrtnXcgOxuuvhoOPhjGjbM+LCefHLqyeFGgiyQgKjsFhfLJJ9CsmfVd+fFHu0LPy4Pzzw9d\nWTxpDF0kQa1axSfAt/nmG5u58swzcOCB8PDDcNNN8JvfhK4s3hToIlJiq1bZBhMDBtjzO+6Au++2\n6YgSngJdRIq1caNt+dazp01HbN0aevSI583fKNMYuojsUn6+reisVQv++U+78Tl9Ojz9tMI8ihTo\nIrJTubk29fCqq6yN7dtv21e9eqErk11RoItIEbNnw0UXwbnnwvLlduNz+nTrwyLRpkAXEcA2YL7u\nOqhbFyZNgr594fPPbbx8DyVFWtBNUZGY+/lnePBBa5qVn29j5ffcYwuEJL0o0EViatMmGDzY+qz8\n8IPNpe/Z01oAS3rSBymRmPEeXnwRTjgBbrvNbnJOm2btbRXm6U2BLhIj770HDRrAlVfCPvtYv5Vx\n42wHIUl/CnSRGPj0U2jeHM45B5Yuhaeesh2Emja1HjSSGRToUibadi09fPst3HCDdT2cOBF697aZ\nK23aQIUKoauTZNNNUSk1bbsWfatXQ58+0K8fbNliY+WdOsHvfhe6MilPukKXUtO2a9G1aRM89hgc\nd5z1ZL/sMpg/34JdYZ75FOhSatp2LXq8t307TzwRbr0VTjoJpk6F556DY48NXZ2kigJdSk3brkXL\nxInQsCFccYVtvvzGGzB+vDXSknhRoEupadu1aJg3Dy69FM46y5btDxtmOwj94Q+auRJXCnQptbhv\nuxba0qXwt79BnTrw7rtw//02c+XaazVzJe40y0XKJI7broW2Zo01zHr4Ydi82cbKO3eGypVDVyZR\nUewVunNumHNuuXNuznbHujnnvnXOzSz4+kP5likSX5s3225B1avbLkHNm9twS//+CnMpqiRDLk8B\nzXZy/BHvfb2CrzeTW1YhLWApSuejUKafC+9h9GibuXLzzdZ7ZcoUeOEFm5YosqNih1y89+8757LK\nv5Rf0wKWonQ+CmX6ufjwQ7jzTvjoI6hdG/77X9t0Qjc7ZXcSuSl6q3NuVsGQTLns+a0FLEXpfBTK\n1HMxfz5cfjk0bgxffw1PPmkzVy6+WGEuxStroA8CjgXqAUuBh3f1QudcW+dcnnMub8WKFaV6Ey1g\nKUrno1CmnYvvv4d27Wzmyvjx1pd8wQLrw1JRUxekhMoU6N77Zd77rd77fOBJ4PTdvHaI9z7be59d\npUqVUr2PFrAUpfNRKFPOxdq1cN99dsPz3/+2UP/iC/uksc8+oauTdFOmQHfOHb7d08uBObt6bSK0\ngKUonY9C6X4uNm+23YKqV4du3Wwx0Ny5MHAgHHJI6OokbXnvd/sFPI8Nq2wGlgDXA88Cs4FZwGvA\n4cX9Od576tev70trxAjvq1Xz3jl7HDGi1H9ERtH5KJSO5yI/3/sxY7yvWdN78P73v/f+o49CVyVR\nB+T5EmSss9emRnZ2ts/Ly0vZ+4lEyaRJNnNl0iSbgti7t80p181OKY5zbpr3vtjuPFr6L1LOPv8c\nWrSARo3gyy+tTcKsWXDJJQpzSS4Fukg5WbbMFgTVrg1jx0L37nbD88YbNXNFyof+Wokk2dq1tqFE\n376wYYM10uraFQ49NHRlkukU6CJJsmWLtbC9916bV96ihXVCrFEjdGUSFwp0kQR5b0vz77rLVno2\namQ9WM48M3RlEjcaQxdJwJQpcPbZttFEfj6MGWM7CCnMJQQFukgZfPGFbfl2xhk2i2XwYPj0U9uU\nWTNXJBQNuYiUwvLl1pN88GDbv7NbN2jfHvbdN3RlIgp0kRJZtw4eeQQefNC+v/FGu/l52GGhKxMp\npEAX2Y0tW+Dpp23a4Xff2ZDKAw9ArVqhKxP5NY2hi+yE9/DGG1CvnrWwrVrVbnaOGaMwl+hSoIvs\nYOpUyMmxTSU2bYKXXrL+K40bh65MZPcU6CIFFi6EK6+E00+3VraPP24zV1q00MwVSQ8aQ5fYW7nS\ndgh64gnYc0/o0gXuuAP23z90ZSKlo0CX2Fq/HgYMsJuca9fC9dfbNMQjjghdmUjZKNAldrZuhWee\nsZkrS5ZYG9sHHrCuiCLpTGPoEhvew//+r81cue46uxJ/7z149VWFuWQGBbrEwrRpcP75tnfn+vUw\nahRMngxnnRW6MpHkUaBLRvvqK2jVCrKzbZegRx+1GSwtW2rmimQejaFLRvrhB+tF/thjUKECdOoE\nHTpo5opkNgW6ZJT162HgQAvzNWvg2mvhvvvgyCNDVyZS/hTokhG2boWRI6FzZ/jmG7joIujdG+rU\nCV2ZSOpoDF3S3tixUL8+tGlj+3ZOmACvv64wl/hRoEvamjEDLrgAmjaF1avh+edtB6GcnNCViYSh\nQJe0s2gRtG5tV+XTplmf8nnzrA/LHvobLTGmMXRJGz/9ZDc7H33Ugvuuu+zrwANDVyYSDQp0ibwN\nG6zzYa9esGqVjZV37w5HHx26MpFo0QdUiaz8fBgxAmrWtO6HZ5wBM2fC8OEKc5GdUaBLJL3zjq3u\nbN0aKle252++CSefHLoykehSoEukfPIJNGsGTZrAjz/a3PKpU+G880JXJhJ9CnSJhMWLbWz8lFPg\n44/h4Yfhs8/gqqs0c0WkpHRTVIJatcp6kQ8YYM/vvBM6doSDDgpbl0g6UqBLEBs3Fs5c+eknGyvv\n0QOqVg1dmUj60odZSan8fHjuOahVC9q3h9NOsxWfTz+tMBdJlAJdUmbCBAvwVq1sMdDYsfDWW1C3\nbujKRDKDAl3K3ezZtlPQeefBypXw7LO2ZL9Jk9CViWQWBbqUmyVLrB953brw0Ufw0EM2c+XqqzVz\nRaQ86KaoJN3PP1sv8v79bcy8fXu4+244+ODQlYlktmKvk5xzw5xzy51zc7Y7drBzbpxzbkHBY8ZP\nMhs5ErKy7MoyK8ueS1EbN9r0w+OOs0D/05/sirxvX4W5SCqU5IPvU0CzHY51BMZ7748Hxhc8z1gj\nR0Lbtta21Xt7bNtWob5Nfj68+CKccALcfjvUqwfTp9tYeVZW6OpE4qPYQPfevw/8uMPhS4GnC75/\nGrgsyXVFSqdOsG5d0WPr1tnxuHv3XWjQwHqR77efzVoZN85WfIpIapX11tSh3vulBd9/Dxy6qxc6\n59o65/Kcc3krVqwo49uFtXhx6Y7HwZw5cPHFtjvQsmU2j3z6dNs9yLnQ1YnEU8JzDbz3HvC7+fUh\n3vts7312lSpVEn27IHa14CWOC2G+/RZuuMFmrnzwATz4oI2T//WvUKFC6OpE4q2sgb7MOXc4QMHj\n8uSVFD29ekGlSkWPVapkx+Pi559tiOn4421s/LbbYOFC6NABfvvb0NWJCJQ90F8D2hR83wZ4NTnl\nRFOrVjBkCFSrZsMJ1arZ81atQldW/jZtgoEDoXp12/7t8sth/nzo1w9+97vQ1YnI9pyNmOzmBc49\nD5wDVAaWAfcCrwCjgKrAIuAK7/2ON05/JTs72+fl5SVYsqSC9/DSSzZ/fOFCGyvv29c2ZhaR1HLO\nTfPeZxf3umIXFnnv/7KLX9KWAxnq/fetje3HH0OdOrZTULNmutkpEnVagC3/b+5cuOQSOPtsu/k5\nfLjt4XnhhQpzkXSgQBe++84WSp10Erz3nm04sWABXHONZq6IpBP1comx1attXLxfP9i8GW69FTp3\ntk2ZRST9KNBjaPNmm6Vz332wYoWt8uzVC449NnRlIpIIDbnEiPcwejSceCLccgvUrm03Pp9/XmEu\nkgkU6DHxwQdw5pnWAXGvveD11yE313YQEpHMoEDPcPPnw2WXwe9/b71nhg6FTz6Biy7SzBWRTKNA\nz1Dffw9//7vNI58wwcbIFyyA667TzBUpR3362Ee/7eXm2nEpdwr0DLNmDXTrZkv1hw6Fm26ylZ73\n3PPrfjQiSXfaaXDFFYWhnptrzzW2lxKa5ZIhNm+2AO/WzdrZtmxpvVeqVw9dmcRKTg6MGmUh3q4d\nDBpkz3NyQlcWC7pCT3Pew5gxNrTSrh3UqAGTJ9u/IYW5BJGTY38Ze/SwR4V5yijQ09ikSdC4Mfzx\njzYu/tprttKzQYPQlUms5ebalXmXLva445i6lBsFehr6/HNo0QIaNYIvv7RFQrNmQfPmmrkigW0b\nMx81Crp3Lxx+UainhAI9jSxbZjc5a9eGsWPt38sXX8CNN0JF3Q2RKJg6teiY+bYx9alTw9YVE8X2\nQ08m9UMvm7Vrrd9K376wYQP87W/2afbQXe7kKiKZJGn90CWcLVtg2DC4916bV96ihc1cqVEjdGUi\nEkUK9Ajy3m5wduxoKz0bNYKXX4aGDUNXJiJRpjH0iJk8Gc46y5brew+vvAITJyrMRaR4CvSIWLDA\nFgM1bGjfDx4Mc+bApZdq5oqIlIyGXAJbvtzWXwweDHvvbSs927eHffcNXZmIpBsFeiC//AL9+8OD\nD8K6dTb18N574bDDQlcmIulKgZ5iW7bAU09B166wdKmNlT/wANSqFboyEUl3GkNPEe9tU4m6de1q\nPCvLNp0YM0ZhLiLJoUBPgY8/tgVzzZtbV8TRo+HDD206oohIsijQy9HChbYBc4MGMHcuPP44fPqp\nNdPSzBURSTaNoZeDlStt5sqgQbDnnrZM/847Yb/9QlcmIplMV+hJtG6d3eA87jh47DG45hqbU969\nu8I8Y2nLNYkQBXoSbN0Kw4dbj5V77oFzzoHZs62t7RFHhK5OypW2XJMIUaAnwHt4802oV882Xz7y\nSNtg4tVXrcWtxMD2W6517VrYC1y79EgACvQyysuD886Diy6C9evt3/C2PiwSM9pyTSJCgV5KX30F\nV11ln6hnz4aBA20GS8uWmrkSW9pyTSJCs1xK6IcfoFcvu9lZsSJ06gQdOsD++4euTILafsu1nBz7\n0rCLBKIr9GKsX2/9Vo47DgYMgNatbeZKz54Kc0FbrkmkaAu6Xdi6FUaMgM6dYckSGyvv3Rvq1Ald\nmYjETUm3oNMV+g68h7ffhlNPtXnkhx1mn6pff11hLiLRpkDfzowZcMEF0KwZrFkDL7wAU6bYvHIR\nkahToANffw1XX21X5TNmWJ/yefPgz3+GPXSGRCRNJDTLxTn3NbAG2ApsKckYT5T8+CPcf79NPdxj\nD7j7brjrLjjggNCViYiUXjKmLeZ471cm4c9JmQ0bbPphr17w8882Vt69Oxx1VOjKRETKLlYDCvn5\nNnOlZk3rftiwIcycCcOGKcxFJP0lGugeeMc5N80513ZnL3DOtXXO5Tnn8lasWJHg25XduHFQv77N\nI69cGcaPtz4sJ58crCQRkaRKNNAbe+/rARcCNzvnftXJxHs/xHuf7b3PrlKlSoJvV3ozZ0LTpjZ7\nZdUqeO45W/Nx7rkpL0VEpFwlFOje+28LHpcDY4DTk1FUMixeDG3a2MyVqVOhXz+YPx/+8hfNXBGR\nzFTmaHPO7eOc22/b98AFwJxkFVZWP/1kPVZq1IAXX7Sx8oUL4R//gL33Dl2diEj5SWSWy6HAGGct\nBisCz3nv30pKVWWwcaPt2dmzpw2t/PWvNnOlatVQFYmIpFaZA917/yVQN4m1lEl+vq3o7NTJFgg1\nbWrNtOoGr0xEJLXSejR5/HjrS96qFRx4IIwdC2+9pTAXkXhKy0CfNQsuvBDOPx9WrrS55dOmQZMm\noSsTEQknrQL9m2/g2mttD8/Jk+Ghh+Czz+wKXTNXRCTu0mLHolWrbFy8f38bM2/f3vquHHxw6MpE\nRKIjLQL9lltg5EjriNizJ1SrFroiEZHoSYtA79bNrspPOSV0JSIi0ZUWgV69eugKRESiT7cSRUQy\nhAJdRCRDKNBFRDJEtAO9Tx/IzS16LDfXjks4+rlEj34mQtQD/bTT4IorCv+i5uba89NOC1tX3Onn\nEj36mQiA9z5lX/Xr1/elNmGC95Ure9+liz1OmFD6P0OSTz+X6NHPJGMBeb4EGRvtK3SAnBxo1w56\n9LDHnJzQFQno5xJF+pnEXvQDPTcXBg2CLl3sccdxQgkj9M9FY8a/FvpnIuGV5DI+WV+lHnLZ9hFy\n20fHHZ9LGFH4uUShhijR+choZMSQy9SpMGpU4UfHnBx7PnVq2LriLgo/l23vecUV0LWrPW5fU9xE\n4WciwTkL/9TIzs72eXl5KXs/iYGuXW3MuEsX23NQJAM556Z577OLe120r9BFdkdjxiJFKNAlPW2b\nZz1qlF2Zbxt+UahLjCnQJT1pzFjkVzSGLiIScRpDFxGJGQW6iEiGUKCLiGQIBbqISIZQoKcT9S8R\nkd1QoKcT9bwWkd2oGLoAKYXt+5e0a2erI+Pcv0REitAVerpRz2sR2QUFerpR/xIR2QUFejpR/xIR\n2Q0FejpR/xIR2Q31cimJPn1sJsn249W5uRakHTqEq0tEYkG9XJJJ0wVFJA1o2mJJaLqgiKQBXaGX\nlKYLikjEJRTozrlmzrnPnHNfOOc6JquoSNJ0QRGJuDIHunOuAvA4cCFQG/iLc652sgqLFE0XFJE0\nkMgV+unAF977L733m4AXgEuTU1bEaLqgiKSBRG6KHgl8s93zJUCDxMqJqJ1NTczJ0Ti6iERKud8U\ndc61dc7lOefyVqxYUd5vJyISW4kE+rfA0ds9P6rgWBHe+yHe+2zvfXaVKlUSeDsREdmdRAJ9KnC8\nc+4Y59xewJXAa8kpS0RESqvMY+je+y3OuVuAt4EKwDDv/adJq0xEREoloZWi3vs3gTeTVIuIiCQg\npc25nHMrgEVl/O2VgZVJLCfd6XwU0rkoSuejqEw4H9W898XehExpoCfCOZdXkm5jcaHzUUjnoiid\nj6LidD7Uy0VEJEMo0EVEMkQ6BfqQ0AVEjM5HIZ2LonQ+iorN+UibMXQREdm9dLpCFxGR3UiLQI9V\n3/XdcM4d7ZzLdc7Ndc596py7LXRNUeCcq+Ccm+Gcez10LaE55w50zr3knJvvnJvnnGsYuqZQnHP/\nKPh3Msc597xz7jehaypvkQ/0WPVdL94WoL33vjZwBnBzjM/F9m4D5oUuIiIGAG9572sBdYnpeXHO\nHQn8D5Dtva+DrWa/MmxV5S/ygU6c+q4Xw3u/1Hs/veD7Ndg/1iPDVhWWc+4o4CLg36FrCc05dwBw\nFjAUwHu/yXu/KmxVQVUEfuucqwhUAr4LXE+5S4dA31nf9ViHGIBzLgs4BZgStpLg+gMdgPzQhUTA\nMcAKYHjBENS/nXP7hC4qBO/9t8BDwGJgKfCz935s2KrKXzoEuuzAObcvMBq43Xu/OnQ9oTjnLgaW\ne++nha4lIioCpwKDvPenAL8Asbzn5Jw7CPskfwxwBLCPc+7qsFWVv3QI9BL1XY8L59yeWJiP9N6/\nHLqewBoBlzjnvsaG4s51zo0IW1JQS4Al3vttn9pewgI+js4HvvLer/DebwZeBs4MXFO5S4dAV9/1\nAs45h42PzvPe9wtdT2je+7u990d577OwvxcTvPcZfxW2K97774FvnHM1Cw6dB8wNWFJIi4EznHOV\nCv7dnEcMbhAn1D43FdR3vYhGQGtgtnNuZsGxewraGIsA3AqMLLj4+RK4NnA9QXjvpzjnXgKmY7PD\nZhCDFaNaKSoikiHSYchFRERKQIEuIpIhFOgiIhlCgS4ikiEU6CIiGUKBLiKSIRToIiIZQoEuIpIh\n/g9vPTAuIhW2LAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25ed8a19780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "circles = [15, 10, 15, 16, 15, 18, 20, 20]\n",
    "crosses = [3, 0.5, 1.2, 3, 2.5, 6.2, 3, 8.3]\n",
    "\n",
    "def line(m, x, d):\n",
    "    return m*x+d\n",
    "\n",
    "line1 = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    line1.append(line(2,i,3.9))\n",
    "\n",
    "print(line1)\n",
    "\n",
    "plt.plot( range(0, len(circles) ), circles, \"bo\")\n",
    "plt.plot( range(0, len(crosses) ), crosses, \"rx\")\n",
    "\n",
    "plt.plot( range(0, len(line1) ), line1, \"b-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Neural Network - MPLClassifier (Multi-Layer-Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   5. ...,   0.   0.   0.]\n",
      " [  0.   0.   0. ...,  10.   0.   0.]\n",
      " [  0.   0.   0. ...,  16.   9.   0.]\n",
      " ..., \n",
      " [  0.   0.   1. ...,   6.   0.   0.]\n",
      " [  0.   0.   2. ...,  12.   0.   0.]\n",
      " [  0.   0.  10. ...,  12.   1.   0.]]\n",
      "[0 1 2 ..., 8 9 8]\n",
      "89 - 1708\n",
      "Iteration 1, loss = 2.35802140\n",
      "Iteration 2, loss = 2.27463027\n",
      "Iteration 3, loss = 2.19906886\n",
      "Iteration 4, loss = 2.13056365\n",
      "Iteration 5, loss = 2.06828304\n",
      "Iteration 6, loss = 2.01136711\n",
      "Iteration 7, loss = 1.95897288\n",
      "Iteration 8, loss = 1.91033267\n",
      "Iteration 9, loss = 1.86478999\n",
      "Iteration 10, loss = 1.82181225\n",
      "Iteration 11, loss = 1.78098164\n",
      "Iteration 12, loss = 1.74197531\n",
      "Iteration 13, loss = 1.70453739\n",
      "Iteration 14, loss = 1.66845224\n",
      "Iteration 15, loss = 1.63352018\n",
      "Iteration 16, loss = 1.59954294\n",
      "Iteration 17, loss = 1.56632213\n",
      "Iteration 18, loss = 1.53367331\n",
      "Iteration 19, loss = 1.50144360\n",
      "Iteration 20, loss = 1.46952863\n",
      "Iteration 21, loss = 1.43788528\n",
      "Iteration 22, loss = 1.40653110\n",
      "Iteration 23, loss = 1.37552681\n",
      "Iteration 24, loss = 1.34494856\n",
      "Iteration 25, loss = 1.31486531\n",
      "Iteration 26, loss = 1.28533098\n",
      "Iteration 27, loss = 1.25638611\n",
      "Iteration 28, loss = 1.22806040\n",
      "Iteration 29, loss = 1.20037281\n",
      "Iteration 30, loss = 1.17333332\n",
      "Iteration 31, loss = 1.14694745\n",
      "Iteration 32, loss = 1.12121900\n",
      "Iteration 33, loss = 1.09614944\n",
      "Iteration 34, loss = 1.07173565\n",
      "Iteration 35, loss = 1.04796755\n",
      "Iteration 36, loss = 1.02482667\n",
      "Iteration 37, loss = 1.00228691\n",
      "Iteration 38, loss = 0.98031713\n",
      "Iteration 39, loss = 0.95888384\n",
      "Iteration 40, loss = 0.93795219\n",
      "Iteration 41, loss = 0.91748489\n",
      "Iteration 42, loss = 0.89744161\n",
      "Iteration 43, loss = 0.87778129\n",
      "Iteration 44, loss = 0.85846741\n",
      "Iteration 45, loss = 0.83947357\n",
      "Iteration 46, loss = 0.82078652\n",
      "Iteration 47, loss = 0.80240588\n",
      "Iteration 48, loss = 0.78434158\n",
      "Iteration 49, loss = 0.76661030\n",
      "Iteration 50, loss = 0.74923151\n",
      "Iteration 51, loss = 0.73222374\n",
      "Iteration 52, loss = 0.71560174\n",
      "Iteration 53, loss = 0.69937491\n",
      "Iteration 54, loss = 0.68354689\n",
      "Iteration 55, loss = 0.66811549\n",
      "Iteration 56, loss = 0.65307271\n",
      "Iteration 57, loss = 0.63840513\n",
      "Iteration 58, loss = 0.62409529\n",
      "Iteration 59, loss = 0.61012362\n",
      "Iteration 60, loss = 0.59647015\n",
      "Iteration 61, loss = 0.58311530\n",
      "Iteration 62, loss = 0.57004047\n",
      "Iteration 63, loss = 0.55722909\n",
      "Iteration 64, loss = 0.54466789\n",
      "Iteration 65, loss = 0.53234783\n",
      "Iteration 66, loss = 0.52026412\n",
      "Iteration 67, loss = 0.50841571\n",
      "Iteration 68, loss = 0.49680448\n",
      "Iteration 69, loss = 0.48543433\n",
      "Iteration 70, loss = 0.47431039\n",
      "Iteration 71, loss = 0.46343806\n",
      "Iteration 72, loss = 0.45282193\n",
      "Iteration 73, loss = 0.44246464\n",
      "Iteration 74, loss = 0.43236583\n",
      "Iteration 75, loss = 0.42252176\n",
      "Iteration 76, loss = 0.41292589\n",
      "Iteration 77, loss = 0.40357054\n",
      "Iteration 78, loss = 0.39444906\n",
      "Iteration 79, loss = 0.38555732\n",
      "Iteration 80, loss = 0.37689442\n",
      "Iteration 81, loss = 0.36846232\n",
      "Iteration 82, loss = 0.36026435\n",
      "Iteration 83, loss = 0.35230254\n",
      "Iteration 84, loss = 0.34457523\n",
      "Iteration 85, loss = 0.33707637\n",
      "Iteration 86, loss = 0.32979643\n",
      "Iteration 87, loss = 0.32272408\n",
      "Iteration 88, loss = 0.31584760\n",
      "Iteration 89, loss = 0.30915589\n",
      "Iteration 90, loss = 0.30263899\n",
      "Iteration 91, loss = 0.29628836\n",
      "Iteration 92, loss = 0.29009693\n",
      "Iteration 93, loss = 0.28405908\n",
      "Iteration 94, loss = 0.27817055\n",
      "Iteration 95, loss = 0.27242839\n",
      "Iteration 96, loss = 0.26683080\n",
      "Iteration 97, loss = 0.26137679\n",
      "Iteration 98, loss = 0.25606585\n",
      "Iteration 99, loss = 0.25089728\n",
      "Iteration 100, loss = 0.24586967\n",
      "Iteration 101, loss = 0.24098028\n",
      "Iteration 102, loss = 0.23622485\n",
      "Iteration 103, loss = 0.23159760\n",
      "Iteration 104, loss = 0.22709171\n",
      "Iteration 105, loss = 0.22269989\n",
      "Iteration 106, loss = 0.21841485\n",
      "Iteration 107, loss = 0.21422952\n",
      "Iteration 108, loss = 0.21013715\n",
      "Iteration 109, loss = 0.20613182\n",
      "Iteration 110, loss = 0.20221014\n",
      "Iteration 111, loss = 0.19837342\n",
      "Iteration 112, loss = 0.19462876\n",
      "Iteration 113, loss = 0.19098772\n",
      "Iteration 114, loss = 0.18746225\n",
      "Iteration 115, loss = 0.18405740\n",
      "Iteration 116, loss = 0.18076500\n",
      "Iteration 117, loss = 0.17756669\n",
      "Iteration 118, loss = 0.17444411\n",
      "Iteration 119, loss = 0.17138615\n",
      "Iteration 120, loss = 0.16838994\n",
      "Iteration 121, loss = 0.16545805\n",
      "Iteration 122, loss = 0.16259511\n",
      "Iteration 123, loss = 0.15980518\n",
      "Iteration 124, loss = 0.15709024\n",
      "Iteration 125, loss = 0.15444963\n",
      "Iteration 126, loss = 0.15188037\n",
      "Iteration 127, loss = 0.14937811\n",
      "Iteration 128, loss = 0.14693813\n",
      "Iteration 129, loss = 0.14455631\n",
      "Iteration 130, loss = 0.14222950\n",
      "Iteration 131, loss = 0.13995558\n",
      "Iteration 132, loss = 0.13773329\n",
      "Iteration 133, loss = 0.13556196\n",
      "Iteration 134, loss = 0.13344123\n",
      "Iteration 135, loss = 0.13137079\n",
      "Iteration 136, loss = 0.12935009\n",
      "Iteration 137, loss = 0.12737813\n",
      "Iteration 138, loss = 0.12545332\n",
      "Iteration 139, loss = 0.12357362\n",
      "Iteration 140, loss = 0.12173680\n",
      "Iteration 141, loss = 0.11994078\n",
      "Iteration 142, loss = 0.11818390\n",
      "Iteration 143, loss = 0.11646499\n",
      "Iteration 144, loss = 0.11478323\n",
      "Iteration 145, loss = 0.11313795\n",
      "Iteration 146, loss = 0.11152849\n",
      "Iteration 147, loss = 0.10995402\n",
      "Iteration 148, loss = 0.10841353\n",
      "Iteration 149, loss = 0.10690593\n",
      "Iteration 150, loss = 0.10543003\n",
      "Iteration 151, loss = 0.10398472\n",
      "Iteration 152, loss = 0.10256897\n",
      "Iteration 153, loss = 0.10118190\n",
      "Iteration 154, loss = 0.09982270\n",
      "Iteration 155, loss = 0.09849068\n",
      "Iteration 156, loss = 0.09718522\n",
      "Iteration 157, loss = 0.09590569\n",
      "Iteration 158, loss = 0.09465150\n",
      "Iteration 159, loss = 0.09342203\n",
      "Iteration 160, loss = 0.09221668\n",
      "Iteration 161, loss = 0.09103483\n",
      "Iteration 162, loss = 0.08987591\n",
      "Iteration 163, loss = 0.08873935\n",
      "Iteration 164, loss = 0.08762465\n",
      "Iteration 165, loss = 0.08653131\n",
      "Iteration 166, loss = 0.08545885\n",
      "Iteration 167, loss = 0.08440682\n",
      "Iteration 168, loss = 0.08337476\n",
      "Iteration 169, loss = 0.08236223\n",
      "Iteration 170, loss = 0.08136876\n",
      "Iteration 171, loss = 0.08039391\n",
      "Iteration 172, loss = 0.07943723\n",
      "Iteration 173, loss = 0.07849829\n",
      "Iteration 174, loss = 0.07757665\n",
      "Iteration 175, loss = 0.07667190\n",
      "Iteration 176, loss = 0.07578365\n",
      "Iteration 177, loss = 0.07491150\n",
      "Iteration 178, loss = 0.07405507\n",
      "Iteration 179, loss = 0.07321397\n",
      "Iteration 180, loss = 0.07238784\n",
      "Iteration 181, loss = 0.07157631\n",
      "Iteration 182, loss = 0.07077902\n",
      "Iteration 183, loss = 0.06999563\n",
      "Iteration 184, loss = 0.06922578\n",
      "Iteration 185, loss = 0.06846917\n",
      "Iteration 186, loss = 0.06772546\n",
      "Iteration 187, loss = 0.06699437\n",
      "Iteration 188, loss = 0.06627560\n",
      "Iteration 189, loss = 0.06556887\n",
      "Iteration 190, loss = 0.06487391\n",
      "Iteration 191, loss = 0.06419044\n",
      "Iteration 192, loss = 0.06351822\n",
      "Iteration 193, loss = 0.06285700\n",
      "Iteration 194, loss = 0.06220651\n",
      "Iteration 195, loss = 0.06156652\n",
      "Iteration 196, loss = 0.06093680\n",
      "Iteration 197, loss = 0.06031711\n",
      "Iteration 198, loss = 0.05970725\n",
      "Iteration 199, loss = 0.05910699\n",
      "Iteration 200, loss = 0.05851613\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       172\n",
      "          1       0.75      0.91      0.82       173\n",
      "          2       0.98      0.69      0.81       173\n",
      "          3       0.78      0.92      0.85       173\n",
      "          4       0.87      0.97      0.92       168\n",
      "          5       0.82      0.96      0.89       167\n",
      "          6       0.92      0.97      0.94       173\n",
      "          7       0.95      0.97      0.96       167\n",
      "          8       0.85      0.63      0.72       169\n",
      "          9       0.94      0.77      0.84       173\n",
      "\n",
      "avg / total       0.89      0.88      0.87      1708\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nplt.gray() \\nplt.matshow(digits.images[12]) \\nplt.show() \\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#Import Data\n",
    "digits = load_digits()\n",
    "print(digits.data)\n",
    "print(digits.target)\n",
    "\n",
    "X = digits.data \n",
    "Y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.05)\n",
    "\n",
    "print(len(X_train),\"-\",len(X_test))\n",
    "#Create Neural Network\n",
    "neuralnetwork = MLPClassifier(hidden_layer_sizes = (100, ),\n",
    "                              max_iter = 200,\n",
    "                              activation = 'logistic', \n",
    "                              learning_rate = 'adaptive',\n",
    "                              verbose = True)\n",
    "neuralnetwork.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = neuralnetwork.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_prediction))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[12]) \n",
    "plt.show() \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
